MDP-Based RL Agent for Navigation in a 100x100 Grid with Obstacles
Problem Overview
We are designing an MDP-based reinforcement learning (RL) agent to navigate a 100x100 grid environment filled with obstacles. The agent must start at one corner of the grid and reach the opposite corner (goal) while avoiding obstacles. The goal is to optimize the agent's policy and actions at each state, ensuring efficient and obstacle-averse movement across the grid. Additionally, we’ll benchmark the performance of Dynamic Programming (DP) methods like Value Iteration with other RL techniques.

Environment Setup
Grid: A 100x100 grid containing obstacles, starting point, and goal.
States: Each cell in the grid represents a unique state, resulting in approximately 10,000 states.
Actions: The agent can move in four directions—up, down, left, or right—unless obstructed by a boundary or obstacle.
Transition Dynamics: The agent transitions based on its current position and the presence of obstacles or boundaries, staying in the same cell if blocked by either.
Rewards:
Goal cell: High positive reward.
Obstacle cells: Penalty for collision.
Step cost: Small penalty per move, encouraging faster goal attainment.
Discount Factor (α): Balances short-term and long-term rewards to encourage direct paths to the goal.
MDP-Based Agent: Policy and Action Learning
To solve this grid navigation problem, we implement two main approaches: Value Iteration and Q-Learning.

Value Iteration:

Method: Calculates the expected value for each cell by evaluating the outcomes of possible moves iteratively. This process converges on optimal values, eventually leading to an optimal policy for the agent.
Advantage: Given full knowledge of the grid, Value Iteration converges quickly, making it an efficient approach for determining the best action in each state.
Q-Learning:

Method: An exploration-based algorithm where the agent learns by trial and error. The agent tests actions, observes the rewards, and iteratively improves its action-value function (Q-values) to maximize the reward.
Advantage: Q-Learning adapts to dynamic environments, handling new or shifting obstacles well, since it learns based on experience rather than a predetermined map.
Benchmarking: Value Iteration vs. Q-Learning
Speed of Convergence:

Value Iteration is faster as it relies on a full understanding of the grid, allowing it to converge relatively quickly.
Q-Learning, in contrast, requires extensive exploration and many iterations to converge, particularly in a large environment like a 100x100 grid.
Adaptability to Changes:

Q-Learning excels in adaptive scenarios, adjusting effectively to changes such as newly introduced obstacles since it constantly updates based on the agent's experience.
Value Iteration, while efficient in a fixed environment, lacks this flexibility and may require re-computation if the environment changes.
Summary
In a fixed grid setup, Value Iteration offers efficiency and rapid convergence, whereas Q-Learning is preferable for environments that may evolve. By comparing these methods, we gain insights into their suitability for dynamic versus static navigation challenges, helping to refine RL strategies in grid-based obstacle environments.
